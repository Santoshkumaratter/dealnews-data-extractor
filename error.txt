DealNews Scraper - Verification Report
=====================================
Date: $(date '+%Y-%m-%d %H:%M:%S %Z')

Summary
- Code and database issues are fixed.
- Deals are upserted (no skip), filters populated, related deals saved.
- CSV and JSON exports enabled.

Environment
- MySQL: localhost:3306, user=root, db=dealnews
- FORCE_UPDATE=true (first run recommended)
- CLEAR_DATA=true (first run recommended)

Run Commands
1) Build & run with logs
   docker-compose build --no-cache scraper
   docker-compose up scraper | tee logs/run_$(date +%F_%H-%M-%S).log

2) Copy internal log
   docker cp dealnews_scraper:/app/dealnews_scraper.log logs/dealnews_scraper.log

3) Exports (auto)
   exports/deals.json
   exports/deals.csv

4) DB dump (optional)
   mysqldump -h localhost -P 3306 -uroot -proot dealnews > dumps/dealnews_$(date +%F_%H-%M-%S).sql

Key Fixes (Applied)
- Removed early return on existing deals; always process and upsert
- ON DUPLICATE KEY UPDATE for deals and deal_filters
- Stable dealid from absolute deallink URL
- deal_filters populated: offer_type, condition_type, events, offer_status, include_expired, category_id, store_id, brand_id, collection_id, start_date, max_price, popularity_rank
- Related deals saved up to 25 per deal

Quick DB Verification
- python3 verify_db.py
  Checks:
  * Deals vs expected count and save ratio
  * deal_filters count and non-null fields
  * Normalized tables populated

Artifacts To Share With Client
- logs/run_YYYY-MM-DD_HH-MM-SS.log
- logs/dealnews_scraper.log
- exports/deals.json (and exports/deals.csv)
- dumps/dealnews_YYYY-MM-DD_HH-MM-SS.sql (optional)

Status
- Ready to send. If older logs show "already exists, skipping" or "auto-id-",
  rebuild with --no-cache and ensure FORCE_UPDATE=true, CLEAR_DATA=true for first run.
