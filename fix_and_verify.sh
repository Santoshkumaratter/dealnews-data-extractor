#!/bin/bash
# Fix and verify DealNews scraper

echo "üîß DealNews Scraper - Fix and Verify"
echo "==================================="

# Check if Docker is available
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker not found. Please install Docker first."
    exit 1
fi

# Stop any running containers
echo "üõë Stopping any running containers..."
docker-compose down -v || docker compose down -v

# Remove old containers and images
echo "üßπ Cleaning up old containers and images..."
docker ps -a | grep dealnews_scraper | awk '{print $1}' | xargs -I{} docker rm -f {} 2>/dev/null || true
docker images | grep dealnews-data-extractor | awk '{print $3}' | xargs -I{} docker rmi -f {} 2>/dev/null || true
docker builder prune -f

# Set up environment
echo "üîß Setting up environment..."
cp .env-template .env 2>/dev/null || cp env.example .env 2>/dev/null || echo "‚ö†Ô∏è No .env template found"

# Force refresh
echo "üîÑ Setting FORCE_UPDATE=true and CLEAR_DATA=true..."
sed -i.bak 's/FORCE_UPDATE=.*/FORCE_UPDATE=true/' .env 2>/dev/null || echo "FORCE_UPDATE=true" >> .env
sed -i.bak 's/CLEAR_DATA=.*/CLEAR_DATA=true/' .env 2>/dev/null || echo "CLEAR_DATA=true" >> .env
rm -f .env.bak 2>/dev/null

# Create directories
echo "üìÅ Creating directories..."
mkdir -p logs dumps

# Rebuild with no cache
echo "üèóÔ∏è Rebuilding with no cache..."
docker-compose build --no-cache scraper || docker compose build --no-cache scraper

# Start the scraper
echo "üöÄ Starting scraper..."
docker-compose up -d scraper || docker compose up -d scraper

# Wait for container to start
echo "‚è≥ Waiting for container to start..."
sleep 5

# Verify container is running
CONTAINER_ID=$(docker ps -q -f name=dealnews_scraper)
if [ -z "$CONTAINER_ID" ]; then
    echo "‚ùå Container not running. Check docker-compose logs."
    exit 1
fi

# Verify the container has the updated code
echo "üîç Verifying container has updated code..."
echo "Checking for 'already exists, skipping' string (should be gone):"
docker exec $CONTAINER_ID sh -c "grep -n 'already exists, skipping' -R /app || echo '‚úÖ OK: String not found'"

echo "Checking for 'deal_' strategy in spider:"
docker exec $CONTAINER_ID sh -c "grep -n 'deal_' /app/dealnews_scraper/spiders/dealnews_spider.py | head -n3"

echo "Checking for 'auto-id' (should NOT exist):"
docker exec $CONTAINER_ID sh -c "grep -n 'auto-id' -R /app || echo '‚úÖ OK: No auto-id found'"

echo "Checking for ON DUPLICATE KEY UPDATE in pipeline:"
docker exec $CONTAINER_ID sh -c "grep -n 'ON DUPLICATE KEY UPDATE' /app/dealnews_scraper/normalized_pipeline.py | head -n2"

# Stream logs for a while
echo "üìú Streaming logs for 60 seconds..."
timeout 60 docker logs -f dealnews_scraper || true

# Copy logs
echo "üìã Copying logs..."
docker cp dealnews_scraper:/app/dealnews_scraper.log logs/dealnews_scraper.log || echo "‚ö†Ô∏è Could not copy log"

# Dump database
echo "üíæ Dumping database..."
mysqldump -h localhost -P 3306 -uroot -proot dealnews > dumps/dealnews_$(date +%F_%H-%M-%S).sql 2>/dev/null || echo "‚ö†Ô∏è Could not dump database"

# Run verification
echo "üîç Running database verification..."
python3 verify_db.py

echo "‚úÖ Done! Check the verification results above."
echo "If verification failed, try running the scraper for longer before verifying."
echo "You can stream logs with: docker logs -f dealnews_scraper"
